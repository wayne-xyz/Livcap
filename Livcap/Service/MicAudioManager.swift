//
//  AudioManager.swift
//  Livcap
//
//  Created by Rongwei Ji on 6/9/25.
//
import Foundation
import AVFoundation
import Accelerate
import OSLog

/// `AudioManager` handles microphone input and audio processing for real-time transcription **on macOS**.
///
/// This class configures an audio engine to capture audio from the system's default input device,
/// converts it to a standard format (16kHz, mono, Float32), and publishes audio chunks as `[Float]` arrays
/// through a Combine publisher.
///
///
///
final class MicAudioManager: ObservableObject {
    
    // MARK: - Configuration
    private let targetSampleRate: Double = 16000.0
    // Removed: dynamic frameBufferSize; tap uses a fixed buffer size for stability

    // MARK: - Published Properties
    @Published private(set) var isRecording = false

    // MARK: - Private Properties
    private var audioEngine: AVAudioEngine?
    private var inputNode: AVAudioInputNode? {
        audioEngine?.inputNode
    }
    
    // VAD Processing
    private let vadProcessor = VADProcessor()
    private var frameCounter: Int = 0
    
    // SystemAudioManager Pattern: Internal raw audio stream
    private var rawAudioStreamContinuation: AsyncStream<AVAudioPCMBuffer>.Continuation?
    private var rawAudioStream: AsyncStream<AVAudioPCMBuffer>?
    
    // Enhanced audio stream with VAD (output)
    private var vadAudioStreamContinuation: AsyncStream<AudioFrameWithVAD>.Continuation?
    private var vadAudioStream: AsyncStream<AudioFrameWithVAD>?
    
    // Structured processing task
    private var audioProcessingTask: Task<Void, Never>?
    
    // Device monitoring
    private let audioMonitor = AudioDeviceMonitor()
    private var isReconfiguring = false
    private var reconfigDebounceWorkItem: DispatchWorkItem?
    
    // Processing context (reused converter and formats)
    private struct ProcessingContext {
        let targetFormat: AVAudioFormat
        let converter: AVAudioConverter
    }
    private var processingContext: ProcessingContext?
    
    // Logging
    private let logger = Logger(subsystem: "com.livcap.microphone", category: "MicAudioManager")

    // logger on/off switch
    private var isLoggerOn: Bool = false //change to true for debugging

    // MARK: - Initialization
    init() {
        self.audioEngine = AVAudioEngine()
        setupDeviceMonitoring()
    }
    
    deinit {
        forceCleanup()
    }

    // MARK: - Public Methods
    func start() async {
        let granted = await requestMicrophonePermission()
        guard granted else {
            logger.warning("macOS Microphone permission denied.")
            return
        }

        await MainActor.run {
            startRecording()
        }
    }
    
    func stop() {
        forceCleanup()
    }
    
    // MARK: - Stream Interface (SystemAudioManager Pattern)
    func audioFramesWithVAD() -> AsyncStream<AudioFrameWithVAD> {
        if let stream = vadAudioStream {
            logger.info("********************Returning existing vadAudioStream")
            return stream
        }
        
        logger.info("@@@@@@@@@@@@@@@@@@@@Creating new vadAudioStream")
        
        // ‚úÖ Create local variable first, avoid self capture in closure
        let stream = AsyncStream<AudioFrameWithVAD> { continuation in
            self.vadAudioStreamContinuation = continuation
            continuation.onTermination = { @Sendable _ in
                // ‚úÖ Don't capture self to avoid retain cycle
                print("üõë MicAudioManager VAD stream terminated")
            }
        }
        
        self.vadAudioStream = stream
        return stream
    }

    // MARK: - Core Audio Logic (SystemAudioManager Pattern)
    
    @MainActor
    private func startRecording() {
        guard !isRecording else {
            logger.warning("Already recording.")
            return
        }
        
        // ‚úÖ Clean up first WITHOUT changing isRecording
        cleanupStreamsAndTasks()
        
        logger.info("üî¥ STARTING MICROPHONE CAPTURE")
        
        do {
            try setupMicrophoneCapture()
            
            isRecording = true
            logger.info("‚úÖ MICROPHONE CAPTURE STARTED")
            
        } catch {
            logger.error("‚ùå Failed to start microphone capture: \(error.localizedDescription)")
            cleanupStreamsAndTasks()
        }
    }
    
    // ‚úÖ Setup without calling cleanup() that changes state
    private func setupMicrophoneCapture() throws {
        // Ensure we have a fresh engine
        if audioEngine == nil { audioEngine = AVAudioEngine() }
        guard let engine = audioEngine else { throw AudioError.formatCreationFailure }

        // Force-create I/O nodes in the graph
        let inputNode = engine.inputNode
        _ = engine.outputNode

        // Create internal raw audio stream (like SystemAudioManager)
        let rawStream = AsyncStream<AVAudioPCMBuffer> { continuation in
            self.rawAudioStreamContinuation = continuation
            continuation.onTermination = { @Sendable _ in
                print("üõë Raw audio stream terminated")
            }
        }
        self.rawAudioStream = rawStream

        // Install tap BEFORE starting the engine; use node's native format (nil)
        inputNode.removeTap(onBus: 0)
        let bufferFrames: AVAudioFrameCount = 2048 // stable across devices
        inputNode.installTap(
            onBus: 0,
            bufferSize: bufferFrames,
            format: nil
        ) { [weak self] buffer, time in
            self?.rawAudioStreamContinuation?.yield(buffer)
        }

        // Prepare and start engine now that graph is complete
        engine.prepare()
        try engine.start()

        // Log final hardware format
        let hwFormat = inputNode.outputFormat(forBus: 0)
        logger.info("üìä Microphone format: \(hwFormat.sampleRate)Hz, \(hwFormat.channelCount) channels")

        // Build/rebuild processing context (reused converter)
        if let target = AVAudioFormat(
            commonFormat: .pcmFormatFloat32,
            sampleRate: targetSampleRate,
            channels: 1,
            interleaved: false
        ), let converter = AVAudioConverter(from: hwFormat, to: target) {
            processingContext = ProcessingContext(targetFormat: target, converter: converter)
            logger.info("üîß ProcessingContext created (\(Int(hwFormat.sampleRate))Hz ‚Üí 16000Hz)")
        } else {
            logger.error("Failed to create ProcessingContext converter")
        }

        logger.info("‚úÖ Tap installed and engine started, installing processing task")

        // Start structured processing
        processMicrophoneOutput(audioStream: rawStream)
    }
    
    // SystemAudioManager Pattern: Structured concurrency processing
    private func processMicrophoneOutput(audioStream: AsyncStream<AVAudioPCMBuffer>) {
        audioProcessingTask = Task {
            logger.info("üéØ Audio processing task started")
            
            for await buffer in audioStream {
                guard !Task.isCancelled else {
                    logger.info("üõë Audio processing task cancelled")
                    break
                }
                // Heavy processing in structured task
                await processMicrophoneBuffer(buffer)
            }
            
            logger.info("üèÅ Audio processing task ended")
        }
    }
    
    // SystemAudioManager Pattern: Buffer processing
    private func processMicrophoneBuffer(_ buffer: AVAudioPCMBuffer) async {
        // Convert to target format using persistent converter
        let convertedBuffer = convertWithProcessingContext(buffer) ?? buffer
        
        // VAD processing
        frameCounter += 1
        let vadResult = vadProcessor.processAudioBuffer(convertedBuffer)
        
        
        // Visualize RMS energy as a bar: longer bar = higher energy
        if isLoggerOn {

            let maxBarLength = 30
            let normalizedEnergy = min(max(Double(vadResult.rmsEnergy) / 0.2, 0), 1) // 0.2 is a typical speech RMS, adjust as needed
            let barLength = max(3, Int(Double(maxBarLength) * normalizedEnergy))
            let bar = String(repeating: "=", count: barLength)
            let now = Date()
            let formatter = DateFormatter()
            formatter.dateFormat = "HH:mm:ss.SSS"
            let timeString = formatter.string(from: now)
            logger.info("Micphone RMS Energy: \(vadResult.rmsEnergy) [\(bar)] at \(timeString)")
        }
        
        // Create enhanced frame
        let audioFrame = AudioFrameWithVAD(
            buffer: convertedBuffer,
            vadResult: vadResult,
            source: .microphone,
            frameIndex: frameCounter
        )
        
        // Yield to output stream
        vadAudioStreamContinuation?.yield(audioFrame)
        
        // Debug log every 100 frames (~10 seconds)
        if frameCounter % 100 == 0 {
            logger.info("üì¶ Processed \(self.frameCounter) microphone frames")
        }
    }
    
    // MARK: - Cleanup (Fixed)
    
    // ‚úÖ Public cleanup that respects state
    private func forceCleanup() {
        logger.info("üõë FORCE CLEANUP MICROPHONE")
        
        // 1Ô∏è‚É£ Always remove tap, then stop audio hardware
        inputNode?.removeTap(onBus: 0)
        if let engine = audioEngine, engine.isRunning {
            engine.stop()
            logger.info("üõë Audio engine stopped")
        }
        
        // 2Ô∏è‚É£ Cancel and clean up streams/tasks
        cleanupStreamsAndTasks()
        
        // 3Ô∏è‚É£ Update state
        Task { @MainActor in
            self.isRecording = false
        }
        
        // 4Ô∏è‚É£ Reset processing state
        vadProcessor.reset()
        frameCounter = 0
        
        logger.info("‚úÖ MICROPHONE CLEANUP COMPLETED")
    }
    
    // ‚úÖ Stream and task cleanup without state changes
    private func cleanupStreamsAndTasks() {
        logger.info("üßπ Cleaning up streams and tasks")
        
        // Cancel structured task
        audioProcessingTask?.cancel()
        audioProcessingTask = nil
        
        // Clean raw audio stream
        rawAudioStreamContinuation?.finish()
        rawAudioStreamContinuation = nil
        rawAudioStream = nil
        
        // Clean VAD stream
        vadAudioStreamContinuation?.finish()
        vadAudioStreamContinuation = nil
        vadAudioStream = nil
        
        // Drop processing context
        processingContext = nil
        
        logger.info("‚úÖ Streams and tasks cleaned")
    }
    
    // MARK: - Device Monitoring Setup
    
    private func setupDeviceMonitoring() {
        audioMonitor.onDeviceChanged { [weak self] defaultInputName in
            guard let self else { return }
            let name = defaultInputName ?? "unknown"
            self.logger.info("üîÑ Default input changed to: \(name)")
            self.scheduleReconfigureAfterDeviceChange()
        }
    }

    private func scheduleReconfigureAfterDeviceChange() {
        // Debounce reconfig to coalesce change storms
        reconfigDebounceWorkItem?.cancel()
        let work = DispatchWorkItem { [weak self] in
            Task { @MainActor in
                self?.performReconfigureForDeviceChange()
            }
        }
        reconfigDebounceWorkItem = work
        DispatchQueue.main.asyncAfter(deadline: .now() + 0.2, execute: work)
    }

    @MainActor
    private func performReconfigureForDeviceChange() {
        guard isRecording else {
            logger.info("üîá Ignoring device change because not recording")
            return
        }
        guard !isReconfiguring else {
            logger.info("‚è≥ Reconfigure already in progress")
            return
        }
        isReconfiguring = true
        logger.info("üõ†Ô∏è Reconfiguring microphone after device change")

        // 1) Stop audio engine quickly (remove tap first)
        inputNode?.removeTap(onBus: 0)
        if let engine = audioEngine, engine.isRunning {
            engine.stop()
            logger.info("üõë Engine stopped for reconfigure")
        }

        // 2) Cancel task and raw stream but keep VAD output alive
        audioProcessingTask?.cancel()
        audioProcessingTask = nil
        rawAudioStreamContinuation?.finish()
        rawAudioStreamContinuation = nil
        rawAudioStream = nil
        // Keep vadAudioStreamContinuation to preserve downstream consumers
        vadProcessor.reset()
        frameCounter = 0

        // 3) Recreate engine for a clean bind to the new device
        audioEngine = AVAudioEngine()

        // 4) Start capture again after a short stabilization delay
        let delay: TimeInterval = 0.35
        DispatchQueue.main.asyncAfter(deadline: .now() + delay) { [weak self] in
            guard let self else { return }
            do {
                try self.setupMicrophoneCapture()
                self.logger.info("‚úÖ Reconfigure completed; capture restarted")
            } catch {
                self.logger.error("‚ùå Reconfigure failed: \(error.localizedDescription)")
            }
            self.isReconfiguring = false
        }
    }
    
    // MARK: - Helpers and Utilities
    
    private func requestMicrophonePermission() async -> Bool {
        switch AVCaptureDevice.authorizationStatus(for: .audio) {
        case .authorized:
            return true
        case .notDetermined:
            return await AVCaptureDevice.requestAccess(for: .audio)
        case .denied, .restricted:
            return false
        @unknown default:
            return false
        }
    }
    
    // Removed legacy per-call conversion; use convertWithProcessingContext instead

    private func convertWithProcessingContext(_ inputBuffer: AVAudioPCMBuffer) -> AVAudioPCMBuffer? {
        guard let context = processingContext else { return nil }
        let srcRate = inputBuffer.format.sampleRate
        let dstRate = context.targetFormat.sampleRate
        let estimatedOutFrames = AVAudioFrameCount(max(1, Int((Double(inputBuffer.frameLength) / srcRate) * dstRate + 16)))
        guard let outBuffer = AVAudioPCMBuffer(pcmFormat: context.targetFormat, frameCapacity: estimatedOutFrames) else {
            logger.error("Failed to allocate output buffer for conversion")
            return nil
        }
        outBuffer.frameLength = estimatedOutFrames

        var error: NSError?
        context.converter.convert(to: outBuffer, error: &error) { inNumPackets, outStatus in
            outStatus.pointee = .haveData
            return inputBuffer
        }
        if let error = error {
            logger.error("Persistent converter error: \(error.localizedDescription)")
            return nil
        }
        return outBuffer
    }
}

// MARK: - Error Types

enum AudioError: Error, LocalizedError {
    case formatCreationFailure

    var errorDescription: String? {
        switch self {
        case .formatCreationFailure:
            return "Failed to create processing audio format."
        }
    }
}
